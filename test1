import sqlite3
import sys
from tqdm import tqdm
from datetime import datetime

# Connect to the original SQLite database
db_name = sys.argv[1]
conn = sqlite3.connect(db_name)

# Create an index on the Rpt_ID column if it doesn't already exist
conn.execute("CREATE INDEX IF NOT EXISTS rpt_id_index ON trades (Rpt_ID)")

# Count the number of rows to update
count_query = f"""
    SELECT COUNT(*) FROM grouped_trades AS g1
    WHERE EXISTS (
        SELECT 1
        FROM grouped_trades AS g2
        WHERE g1.Rpt_ID = g2.Rpt_ID
          AND g2.Pty_R = '18'
          AND g2.ORFInd = 'Y'
    ) AND g1.Pty_R = '1';
"""

# Get the count of rows to update
cursor = conn.execute(count_query)
total_rows_to_update = cursor.fetchone()[0]

# Define the batch size (adjust as needed)
batch_size = 1000

# Calculate the number of batches
num_batches = (total_rows_to_update + batch_size - 1) // batch_size

# Create a progress bar for overall progress
overall_pbar = tqdm(total=num_batches, desc='Updating rows', dynamic_ncols=True)

for batch_num in range(num_batches):
    offset = batch_num * batch_size

    # Update the 'grouped_trades' table for the current batch
    query_update = f"""
        UPDATE grouped_trades AS g1
        SET ORFInd = 'Y', UpdateTimestamp = '{datetime.now()}'
        WHERE EXISTS (
            SELECT 1
            FROM grouped_trades AS g2
            WHERE g1.Rpt_ID = g2.Rpt_ID
              AND g2.Pty_R = '18'
              AND g2.ORFInd = 'Y'
        ) AND g1.Pty_R = '1'
        LIMIT {batch_size};
    """

    # Execute the batch update query
    conn.execute(query_update)
    conn.commit()

    # Update the progress bar for the current batch
    overall_pbar.update(1)
    # Flush the stdout buffer to ensure real-time progress updates
    sys.stdout.flush()

# Close the connection to the database
conn.close()

# Close the progress bar
overall_pbar.close()

print("Data has been successfully updated.")












# Create a new table for the current exchange to store the query results
conn.execute(f'''CREATE TABLE IF NOT EXISTS grouped_trades (
                    Side TEXT,
                    Sub_ID TEXT,
                    Rpt_ID TEXT,
                    Pty_R TEXT,
                    Ultimate_Clearing_Firm TEXT,
                    EXCH TEXT,
                    TRANS_TYPE TEXT,
                    ORFInd TEXT,
                    Quantity INTEGER,  -- Add the Quantity column
                    INDEX rpt_id_index (Rpt_ID)  -- Create an index on Rpt_ID
                )''')






import sqlite3
import sys
from tqdm import tqdm
from datetime import datetime

# Connect to the original SQLite database
db_name = sys.argv[1]
conn = sqlite3.connect(db_name)

# Create a progress bar for overall progress
overall_pbar = tqdm(desc='Updating rows', dynamic_ncols=True)

# Create a new table to store the updated data
conn.execute('DROP TABLE IF EXISTS updated_grouped_trades')
conn.execute(f'''CREATE TABLE IF NOT EXISTS updated_grouped_trades (
                    Side TEXT,
                    Sub_ID TEXT,
                    Rpt_ID TEXT,
                    Pty_R TEXT,
                    Ultimate_Clearing_Firm TEXT,
                    EXCH TEXT,
                    TRANS_TYPE TEXT,
                    ORFInd TEXT,
                    Quantity INTEGER,
                    UpdateTimestamp TEXT  -- Add UpdateTimestamp column
                    )''')

batch_size = 1000  # Adjust as needed
updated_rows = True

while updated_rows:
    # Insert data from grouped_trades into updated_grouped_trades
    insert_query = f"""
        INSERT INTO updated_grouped_trades (Side, Sub_ID, Rpt_ID, Pty_R, Ultimate_Clearing_Firm, EXCH, TRANS_TYPE, ORFInd, Quantity)
        SELECT Side, Sub_ID, Rpt_ID, Pty_R, Ultimate_Clearing_Firm, EXCH, TRANS_TYPE, ORFInd, Quantity
        FROM grouped_trades
        WHERE Pty_R IN ('1', '18')
        LIMIT {batch_size};
    """

    # Execute the insert query
    conn.execute(insert_query)
    conn.commit()

    # Update the 'updated_grouped_trades' table for the current batch
    update_query = f"""
        UPDATE updated_grouped_trades AS g1
        SET ORFInd = 'Y', UpdateTimestamp = '{datetime.now()}'
        WHERE EXISTS (
            SELECT 1
            FROM updated_grouped_trades AS g2
            WHERE g1.Rpt_ID = g2.Rpt_ID
              AND g2.Pty_R = '18'
              AND g2.ORFInd = 'Y'
        ) AND g1.Pty_R = '1';
    """

    # Execute the batch update query
    conn.execute(update_query)
    conn.commit()

    # Check if any rows were updated in the current batch
    rows_updated = conn.total_changes

    # Update the progress bar for the current batch
    overall_pbar.update(1)
    # Flush the stdout buffer to ensure real-time progress updates
    sys.stdout.flush()

    # If no rows were updated in the current batch, exit the loop
    if rows_updated == 0:
        updated_rows = False

# Close the connection to the database
conn.close()

# Close the progress bar
overall_pbar.close()

print("Data has been successfully updated.")















import sqlite3
import sys
from tqdm import tqdm
from datetime import datetime

# Connect to the original SQLite database
db_name = sys.argv[1]
conn = sqlite3.connect(db_name)

# Create an index on the Rpt_ID column if it doesn't already exist
conn.execute("CREATE INDEX IF NOT EXISTS rpt_id_index ON trades (Rpt_ID)")

# Count the number of rows to update
count_query = f"""
    SELECT COUNT(*) FROM grouped_trades AS g1
    WHERE EXISTS (
        SELECT 1
        FROM grouped_trades AS g2
        WHERE g1.Rpt_ID = g2.Rpt_ID
          AND g2.Pty_R = '18'
          AND g2.ORFInd = 'Y'
    ) AND g1.Pty_R = '1';
"""

# Get the count of rows to update
cursor = conn.execute(count_query)
total_rows_to_update = cursor.fetchone()[0]

# Define the batch size (adjust as needed)
batch_size = 1000

# Calculate the number of batches
num_batches = (total_rows_to_update + batch_size - 1) // batch_size

# Create a progress bar for overall progress
overall_pbar = tqdm(total=num_batches, desc='Updating rows', dynamic_ncols=True)

for batch_num in range(num_batches):
    offset = batch_num * batch_size

    # Update the 'grouped_trades' table for the current batch
    query_update = f"""
        UPDATE grouped_trades AS g1
        SET ORFInd = 'Y', UpdateTimestamp = '{datetime.now()}'
        WHERE EXISTS (
            SELECT 1
            FROM grouped_trades AS g2
            WHERE g1.Rpt_ID = g2.Rpt_ID
              AND g2.Pty_R = '18'
              AND g2.ORFInd = 'Y'
        ) AND g1.Pty_R = '1'
        LIMIT {batch_size};
    """

    # Execute the batch update query
    conn.execute(query_update)
    conn.commit()

    # Update the progress bar for the current batch
    overall_pbar.update(1)
    # Flush the stdout buffer to ensure real-time progress updates
    sys.stdout.flush()

# Close the connection to the database
conn.close()

# Close the progress bar
overall_pbar.close()

print("Data has been successfully updated.")




pvellanki@qa2434:~/orsa$ python3 groupedexcel_postmove_v3.py postmove.db 
Traceback (most recent call last):
  File "/home/pvellanki/orsa/groupedexcel_postmove_v3.py", line 13, in <module>
    conn.execute(f'''CREATE TABLE IF NOT EXISTS grouped_trades (
sqlite3.OperationalError: near "INDEX": syntax error






















Traceback (most recent call last):
  File "/home/pvellanki/orsa/groupedexcel_postmove_v3.py", line 13, in <module>
    conn.execute(f'''CREATE TABLE IF NOT EXISTS grouped_trades (
sqlite3.OperationalError: near "INDEX": syntax error
pvellanki@qa2434:~/orsa$ cat postmove.py
import sqlite3
import xml.etree.ElementTree as ET
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import sys
import time

db_name = sys.argv[2]
ns = {'fixml': 'http://www.fixprotocol.org/FIXML-4-4'}

# Function to determine Sub ID
def extract_sub_id(element):
    sub_id_element = element.find('.//fixml:Sub', ns)
    return sub_id_element.get('ID') if sub_id_element is not None else None

# Function to extract attribute values from the XML elements
def extract_attributes(element, attribute_names):
    attributes = [element.get(attr) for attr in attribute_names]
    return tuple(attributes) if len(attributes) == len(attribute_names) else (None,) * len(attribute_names)

def get_ultimate_clearing_firm(sub_id, pty_r, rpt_id, pty_id):
    if pty_r == '14':
        return pty_id
    elif pty_r == '18':
        return pty_id
    elif pty_r == '1':
        return pty_id
    return None



def create_table():
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()

    cursor.execute("DROP TABLE IF EXISTS trades;")

    # Create the table for storing the trade data
    cursor.execute('''CREATE TABLE IF NOT EXISTS trades (
                        Quantity INTEGER,
                        Side TEXT,
                        Pty_ID TEXT,
                        Pty_R TEXT,
                        Sub_ID TEXT,
                        Rpt_ID TEXT,
                        Ultimate_Clearing_Firm TEXT,
                        EXCH,
                        TRANS_TYPE,
                        ORFInd TEXT
                     )''')

    conn.commit()
    conn.close()

# Function to process a batch of data and insert into the database


def process_batch(batch_data):
    conn = None
    cursor = None
    retries = 5

    while retries > 0:
        try:
            conn = sqlite3.connect(db_name)
            cursor = conn.cursor()

            total_batches = len(batch_data)
            with tqdm(total=total_batches, desc='Inserting data') as pbar:

                # Insert the batch data into the database
                for data in batch_data:
                    quantity, side, rpt_id = data[0:3]
                    pty_id, pty_r, sub_id = data[3:6]  # Extract pty_id, pty_r, sub_id from the data tuple
                    trans_type, exch = data[6:8]
                    orfind = data[8]
                    ultimate_clearing_firm = get_ultimate_clearing_firm(sub_id, pty_r, rpt_id, pty_id)
                    
                    

                    cursor.execute('INSERT INTO trades VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)',
                                (quantity, side, pty_id, pty_r, sub_id, rpt_id,
                                    ultimate_clearing_firm,exch,trans_type,orfind ))
                    
                    #pbar.update(1)


            conn.commit()
            conn.close()
            break  # Success, break the loop
        except sqlite3.OperationalError as e:
            if 'database is locked' in str(e):
                retries -= 1
                if retries > 0:
                    print("Database locked. Retrying in 1 second...")
                    time.sleep(2)
            else:
                # If the error is not related to locking, raise it.
                raise
        finally:
            if conn:
                conn.close()


# Function to fetch data in batches from the trades database
def fetch_data_in_batches(xml_file, batch_size):
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()

    create_table()

    # Parse the XML data and extract the required attributes
    print("Parsing the XML data...")
    tree_trade = ET.parse(xml_file)
    root_trade = tree_trade.getroot()

    # Initialize batch_data list
    batch_data = []

    # Loop through the TrdCaptRpt elements and extract the data
    print("Extracting data from the XML...")
    data_to_process = []
    for trd_capt_rpt in tqdm(root_trade.findall('.//fixml:TrdCaptRpt', ns)):
        rpt_id = trd_capt_rpt.get('RptID')
        trans_type = trd_capt_rpt.get('TransTyp')
        exch = trd_capt_rpt.find('fixml:Instrmt', ns).get('Exch')

        for rpt_side in trd_capt_rpt.findall('fixml:RptSide', ns):
            side = rpt_side.get('Side')
            orfind = rpt_side.get('ORFInd') or '' 

            for pty in rpt_side.findall('fixml:Pty', ns):
                pty_id = pty.get('ID')
                pty_r = pty.get('R')
                sub_element = pty.find('fixml:Sub', ns)
                sub_id = sub_element.get('ID') if sub_element is not None else None

                # Append the parsed data to the batch_data list
                batch_data.append((int(trd_capt_rpt.get('LastQty')), side, rpt_id, pty_id, pty_r, sub_id,trans_type,exch,orfind))

                if len(batch_data) >= batch_size:
                    data_to_process.append(batch_data)
                    batch_data = []

    # Insert the remaining batch data into the data_to_process
    if batch_data:
        data_to_process.append(batch_data)

    # Use multiprocessing to process the data
    with Pool(processes=cpu_count()) as pool:
        pool.map(process_batch, data_to_process)

    # Close the cursor and connection
    cursor.close()
    conn.close()

    print("Data has been successfully stored in the SQLite database.")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python your_script.py <xml_file>")
        sys.exit(1)

    xml_file = sys.argv[1]  # Get the XML file name from command-line arguments
    batch_size = 100  # Adjust the batch size as per your requirement
    fetch_data_in_batches(xml_file, batch_size)






cursor.execute("CREATE INDEX IF NOT EXISTS rpt_id_index ON trades (Rpt_ID);")  # Add this line to create an index on Rpt_IDimport sqlite3
import sys
from tqdm import tqdm
from datetime import datetime
from multiprocessing import Pool, cpu_count

# Function to update a batch of rows
def update_batch(batch):
    conn = sqlite3.connect(db_name)
    conn.execute("BEGIN")  # Start a transaction for the batch
    cursor = conn.cursor()

    for rpt_id in batch:
        query_update = f"""
            UPDATE grouped_trades AS g1
            SET ORFInd = 'Y', UpdateTimestamp = '{datetime.now()}'
            WHERE Rpt_ID = '{rpt_id}'
              AND EXISTS (
                SELECT 1
                FROM grouped_trades AS g2
                WHERE g2.Rpt_ID = g1.Rpt_ID
                  AND g2.Pty_R = '18'
                  AND g2.ORFInd = 'Y'
            ) AND g1.Pty_R = '1';
        """
        cursor.execute(query_update)

    conn.commit()
    conn.close()

# Connect to the original SQLite database
db_name = sys.argv[1]
conn = sqlite3.connect(db_name)



# Define the batch size (adjust as needed)
batch_size = 1000

# Count the number of rows to update
count_query = f"""
    SELECT COUNT(DISTINCT Rpt_ID) FROM grouped_trades AS g1
    WHERE EXISTS (
        SELECT 1
        FROM grouped_trades AS g2
        WHERE g1.Rpt_ID = g2.Rpt_ID
          AND g2.Pty_R = '18'
          AND g2.ORFInd = 'Y'
    ) AND g1.Pty_R = '1';
"""

# Get the count of distinct Rpt_IDs to update
cursor = conn.execute(count_query)
total_rows_to_update = cursor.fetchone()[0]

# Calculate the number of batches
num_batches = (total_rows_to_update + batch_size - 1) // batch_size

# Create a list of Rpt_IDs to update
rpt_ids_to_update = conn.execute(f"""
    SELECT DISTINCT Rpt_ID FROM grouped_trades AS g1
    WHERE EXISTS (
        SELECT 1
        FROM grouped_trades AS g2
        WHERE g1.Rpt_ID = g2.Rpt_ID
          AND g2.Pty_R = '18'
          AND g2.ORFInd = 'Y'
    ) AND g1.Pty_R = '1'
    LIMIT {total_rows_to_update};
""").fetchall()

# Close the connection to the database
conn.close()

# Create a progress bar for overall progress
overall_pbar = tqdm(total=num_batches, desc='Updating rows', dynamic_ncols=True)

# Use multiprocessing to update the data in parallel
with Pool(processes=cpu_count()) as pool:
    for _ in pool.imap_unordered(update_batch, [rpt_ids_to_update[i:i+batch_size] for i in range(0, total_rows_to_update, batch_size)]):
        overall_pbar.update(1)

# Close the progress bar
overall_pbar.close()

print("Data has been successfully updated.")

