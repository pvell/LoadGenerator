import sqlite3
import pandas as pd
import glob
import re
import argparse

# Define a command-line argument for the month
parser = argparse.ArgumentParser()
parser.add_argument('--month', required=True, help='The month (YYYY-MM) to consolidate and export.')
args = parser.parse_args()

# Define the regular expression pattern for matching .db files
db_pattern = f'*{args.month}-*.db'

# Specify the output CSV file
output_csv_file = f'Monthly_{args.month}_Trades.csv'

# Connect to the consolidated database
consolidated_conn = sqlite3.connect(':memory:')  # Use an in-memory database for consolidation
consolidated_cursor = consolidated_conn.cursor()

# Create the consolidated data table
consolidated_cursor.execute('''CREATE TABLE consolidated_data AS
                            SELECT * FROM grouped_trades_summary WHERE 0''')

# Iterate through database files matching the pattern
for db_file in glob.glob(db_pattern):
    # Connect to the individual database
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Attach the individual database to the consolidated database
    cursor.execute(f"ATTACH DATABASE '{db_file}' AS {args.month}")

    # Copy data from the individual database to the consolidated database
    query = f"INSERT INTO consolidated_data SELECT * FROM {args.month}.grouped_trades_summary"
    consolidated_cursor.execute(query)

    # Detach the individual database
    cursor.execute(f"DETACH DATABASE {args.month}")
    conn.close()

# Group data by the specified columns
query = """
SELECT Side, Sub_ID, Ultimate_Clearing_Firm, Entering_Firm_Col1, EXCH, TRANS_TYPE, SUM(Total_Quantity)
FROM consolidated_data
GROUP BY Side, Sub_ID, Ultimate_Clearing_Firm, Entering_Firm_Col1, EXCH, TRANS_TYPE
"""

# Execute the query
result = consolidated_cursor.execute(query)

# Convert the result to a pandas DataFrame
df = pd.DataFrame(result.fetchall(), columns=['Side', 'Sub_ID', 'Ultimate_Clearing_Firm', 'Entering_Firm_Col1', 'EXCH', 'TRANS_TYPE', 'Total_Quantity'])

# Export the grouped data to a CSV file
df.to_csv(output_csv_file, index=False)

# Close the consolidated database connection
consolidated_conn.close()

print(f'Data exported to {output_csv_file}')













#!/bin/bash

# Specify the name of the compressed file
compressed_file="file.Z"

# Uncompress the file in the background
uncompress "$compressed_file" &

# Wait for the uncompression process to complete
wait

# Continue with the next steps
echo "Uncompression is complete, proceeding with the next steps."

# Add your additional commands here








https://marketdata.theocc.com/volume-query?reportDate=20221020&format=csv&volumeQueryTy[…]pe=D&accountType=ALL&productKind=ALL&porc=BOTH&contractDt=


https://marketdata.theocc.com/volume-query?reportDate=20231012&format=csv&volumeQueryType=O&symbolType=ALL&symbol=&reportType=D&accountType=ALL&productKind=ALL&porc=BOTH&contractDt=



“https://marketdata.theocc.com/volume-query?reportDate=” + DBTimeUtils.lastBusinessDateNy().replace(“-”,“”) + “&format=csv&volumeQueryType=O&symbolType=ALL&symbol=&reportType=D&accountType=ALL&productKind=ALL&porc=BOTH&contractDt=”










#!/bin/bash


current_date=$(date +"%Y-%m-%d")
date=$(date -d "$current_date - 1 day" +"%Y-%m-%d")

mm_dd=$(date -d "$date" +"%m-%d")

# Create the output directory if it doesn't exist
output_dir="/data/orsa-intermediate"

# Define the directory where the files are located
base_dir="/mnt/occ/downloads/prod"

# Construct the file name using the provided date
file_to_watch="memx_orsa_trades_$date.xml"

# Define the monitor folder
monitor_folder="$base_dir"

# Check if the file exists, wait up to 10 times
retries=10
while [ ! -e "$monitor_folder/$file_to_watch" ] && [ $retries -gt 0 ]; do
    echo "$file_to_watch not found in $monitor_folder. Waiting..."
    sleep 10  # Adjust the interval as needed
    ((retries--))
done

if [ $retries -eq 0 ]; then
    echo "File not found after 10 retries. Exiting."
    exit 1
fi

echo "Found $file_to_watch"

# Check if the XML file is complete by examining the last two lines
last_two_lines=$(tail -n 2 "$monitor_folder/$file_to_watch")

if [[ $last_two_lines != *"</FIXML>"* ]]; then
    echo "XML file is not complete. Missing </FIXML> tag."
    exit 1
fi



# Define a function to execute trades and exchange recon
execute_trades_and_recon() {
    # Add the content of run_trades.sh here
    echo "Executing run_trades.sh..."
    db_name="$date.db"

    # Step 1: Split the big XML file using split.py
    python3 split_v4.py "$base_dir"/memx_orsa_trades_"$date".xml "$date"

    # Step 2: Process each split file using xml_to_db.py
    for split_file in "$date"_*; do
        python3 xml_2_db_v5.py "$split_file" "$db_name" > log.txt &

        # Capture the PID of the last background process
        pid=$!

        # Wait for the background process to complete
        wait "$pid"

        echo "Processing of $split_file completed."
    done

    # Step 3: Group the data in the database using groupedexcel.py
    python3 groupedexcel_v5.py "$db_name"

    grouped_pid=$!
    # Wait for groupedexcel.py to complete
    wait "$grouped_pid"

    echo "Grouping of data completed."

    # Step 4: Perform further processing using furtherstreaming.py
    python3 streamfurther_v4.py "$db_name"

    streamfurther_pid=$!
    # Wait for groupedexcel.py to complete
    wait "$streamfurther_pid"

    echo "Grouping summary of data completed."


    # Step 4: Perform further processing using furtherstreaming.py
    python3 dump2excel.py "$db_name"

    dump2excel_pid=$!
    # Wait for groupedexcel.py to complete
    wait "$dump2excel_pid"

    echo "Grouping summary csv generated."

    
    echo "Executing run_exchange_recon.sh..."
    db_name="$date.db"



    # Step 1: Group the data in the database using groupedexcel.py
    python3 grouptradesbyexch.py "$db_name"

    groupedbyexch_pid=$!
    # Wait for groupedexcel.py to complete
    wait "$groupedbyexch_pid"

    echo "Grouping by exchange completed."

    # Step 4: Perform further processing using furtherstreaming.py
    python3 streamfurther_v5.py "$db_name"

    SF5_pid=$!
    wait "$SF5_pid"

    echo "grouping summary bu exchange completed"

    python3 exchangrecon.py "$db_name"

    exchangerecon_pid=$!
    wait "$exchangerecon_pid"

    echo "exchange recon against OCC Completed"

    python3 excel.py "$db_name"

    excel_pid=$!
    wait "$excel_pid"

    echo "Common aggregates file generated"
}

# Execute the combined trades and exchange recon function
execute_trades_and_recon

echo "Scripts executed successfully."

mv *"$mm_dd.csv" "$output_dir/"
