import sys
from tqdm import tqdm
from datetime import datetime
from multiprocessing import Pool, cpu_count
import sqlite3

# Function to update a batch of rows
def update_batch(batch):
    conn = sqlite3.connect(db_name)
    conn.execute("BEGIN")  # Start a transaction for the batch
    cursor = conn.cursor()

    for rpt_id in batch:
        query_update = """
            UPDATE grouped_trades AS g1
            SET ORFInd = 'Y'
            WHERE Rpt_ID = ?
            AND EXISTS (
                SELECT 1
                FROM grouped_trades AS g2
                WHERE g2.Rpt_ID = g1.Rpt_ID
                AND g2.Pty_R = '18'
                AND g2.ORFInd = 'Y'
            ) AND g1.Pty_R = '1';
        """

        # Now, execute the query with parameters
        cursor.execute(query_update, rpt_id)
      
    conn.commit()
    conn.close()

# Connect to the original SQLite database
db_name = sys.argv[1]
conn = sqlite3.connect(db_name)
conn.execute('pragma journal_mode=wal')
conn.execute('pragma synchronous=normal')
conn.execute('pragma temp_store=memory')
conn.execute('pragma wal_autocheckpoint=10000')

# Define the batch size (adjust as needed)
batch_size = 1000

# Count the number of rows to update
count_query = f"""
    SELECT COUNT(DISTINCT g1.Rpt_ID)
    FROM grouped_trades AS g1
    INNER JOIN (
        SELECT DISTINCT Rpt_ID
        FROM grouped_trades
        WHERE Pty_R = '18' AND ORFInd = 'Y'
    ) AS g2 ON g1.Rpt_ID = g2.Rpt_ID
    WHERE g1.Pty_R = '1';
"""

# Get the count of distinct Rpt_IDs to update
cursor = conn.execute(count_query)
total_rows_to_update = cursor.fetchone()[0]


# Calculate the number of batches
num_batches = (total_rows_to_update + batch_size - 1) // batch_size

# Create a list of Rpt_IDs to update
rpt_ids_to_update = conn.execute(f"""
    SELECT DISTINCT g1.Rpt_ID
    FROM grouped_trades AS g1
    INNER JOIN (
        SELECT DISTINCT Rpt_ID
        FROM grouped_trades
        WHERE Pty_R = '18' AND ORFInd = 'Y'
    ) AS g2 ON g1.Rpt_ID = g2.Rpt_ID
    WHERE g1.Pty_R = '1'
    LIMIT {total_rows_to_update};
""").fetchall()

# Close the connection to the database
conn.close()

# Create a progress bar for overall progress
overall_pbar = tqdm(total=num_batches, desc='Updating rows', dynamic_ncols=True)

# Use multiprocessing to update the data in parallel
with Pool(processes=10) as pool:
    for _ in pool.imap_unordered(update_batch, [rpt_ids_to_update[i:i+batch_size] for i in range(0, total_rows_to_update, batch_size)]):
        overall_pbar.update(1)

















 retry_attempts = 3  # Number of retry attempts
        while retry_attempts > 0:
            try:
                # Now, execute the query with parameters
                cursor.execute(query_update, rpt_id)
                conn.commit()
                break  # If successful, exit the retry loop
            except sqlite3.OperationalError as e:
                # Handle database lock error
                if "database is locked" in str(e):
                    print("Database is locked. Retrying...")
                    time.sleep(1)  # Wait for 1 second before retrying
                    retry_attempts -= 1
                    if retry_attempts == 0:
                        print("Retry attempts exhausted. Exiting.")
                        conn.rollback()  # Rollback the transaction
                        conn.close()
                        return  # Exit the function
                else:
                    raise  # If it's not a lock error, raise the exception


# Close the progress bar
overall_pbar.close()

print("Data has been successfully updated.")
