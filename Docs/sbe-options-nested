
import sqlite3
import xml.etree.ElementTree as ET
from multiprocessing import Pool, cpu_count
from tqdm import tqdm  # Import tqdm for the progress bar

# Function to determine Ultimate Clearing Firm
def get_ultimate_clearing_firm(sub_id, pty_r, rpt_id, sub_id_to_rpt_ids):
    if sub_id in ['C', 'M', 'F']:
        for i in range(len(pty_r_list)):
            if pty_r_list[i] == '14' and rpt_id_list[i] == rpt_id:
                return pty_id_list[i] if pty_id_list[i] else None
    return None

# Function to determine Entering Firm - Column 1
def get_entering_firm_col1(sub_id, pty_r, rpt_id, sub_id_to_rpt_ids):
    if sub_id in ['C', 'M', 'F'] and rpt_id in rpt_id_list:
        for i in range(len(pty_r_list)):
            if pty_r_list[i] == '18' and rpt_id_list[i] == rpt_id:
                return pty_id_list[i] if pty_id_list[i] else None
        for i in range(len(pty_r_list)):
            if pty_r_list[i] == '1' and rpt_id_list[i] == rpt_id:
                return pty_id_list[i] if pty_id_list[i] else None
    return None

# Function to determine Entering Firm - Column 2
def get_entering_firm_col2(sub_id, pty_r, rpt_id, sub_id_to_rpt_ids):
    if sub_id in ['C', 'M', 'F'] and rpt_id in rpt_id_list:
        for i in range(len(pty_r_list)):
            if pty_r_list[i] == '2' and rpt_id_list[i] == rpt_id:
                return pty_id_list[i] if pty_id_list[i] else None
        for i in range(len(pty_r_list)):
            if pty_r_list[i] == '26' and rpt_id_list[i] == rpt_id:
                return pty_id_list[i] if pty_id_list[i] else None
    return None



# Function to extract attribute values from the XML elements
def extract_attributes(element, attribute_names):
    return [element.get(attr) for attr in attribute_names]

# Function to process a batch of data and add additional columns
def process_batch(batch_data):
    ultimate_clearing_firm_list = []
    entering_firm_col1_list = []
    entering_firm_col2_list = []

    for row in batch_data:
        # Extract relevant columns from the row
        rpt_id, sub_id, pty_r = row['Rpt_ID'], row['Sub_ID'], row['Pty_R']

        # Determine Ultimate Clearing Firm, Entering Firm - Column 1, and Entering Firm - Column 2
        ultimate_clearing_firm = get_ultimate_clearing_firm(sub_id, pty_r, rpt_id, sub_id_to_rpt_ids)
        entering_firm_col1 = get_entering_firm_col1(sub_id, pty_r, rpt_id, sub_id_to_rpt_ids)
        entering_firm_col2 = get_entering_firm_col2(sub_id, pty_r, rpt_id, sub_id_to_rpt_ids)

        # Append the results to the corresponding lists
        ultimate_clearing_firm_list.append(ultimate_clearing_firm)
        entering_firm_col1_list.append(entering_firm_col1)
        entering_firm_col2_list.append(entering_firm_col2)

    # Add the new columns to the batch_data dictionary
    batch_data['Ultimate Clearing Firm'] = ultimate_clearing_firm_list
    batch_data['Entering Firm - Column 1'] = entering_firm_col1_list
    batch_data['Entering Firm - Column 2'] = entering_firm_col2_list

    return batch_data


# Function to fetch data in batches from the trades database
def fetch_data_in_batches(batch_size):
    # Connect to the SQLite database
    conn = sqlite3.connect('trades.db')
    cursor = conn.cursor()

    # Execute the SELECT query to fetch data in batches
    cursor.execute('SELECT * FROM trade_data')

    total_rows = cursor.rowcount
    progress_bar = tqdm(total=total_rows, desc="Processing", unit="rows")

    while True:
        batch_data = cursor.fetchmany(batch_size)
        if not batch_data:
            break

        # Process the batch in parallel using multiprocessing
        with Pool(processes=cpu_count()) as pool:
            processed_batches = list(tqdm(pool.imap_unordered(process_batch, batch_data), total=len(batch_data), desc="Batch", unit="rows"))

        # Update the database with the processed data
        for processed_batch in processed_batches:
            # Your logic here to insert the processed batch data back into the database
            # Adjust the following code based on your table structure and data
            for row in processed_batch:
                cursor.execute('INSERT INTO new_trade_data (Quantity, Side, Pty_ID, Pty_R, Sub_ID, Rpt_ID, '
                               'Ultimate_Clearing_Firm, Entering_Firm_Column1, Entering_Firm_Column2) '
                               'VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)',
                               (row['Quantity'], row['Side'], row['Pty_ID'], row['Pty_R'], row['Sub_ID'],
                                row['Rpt_ID'], row['Ultimate Clearing Firm'], row['Entering Firm - Column 1'],
                                row['Entering Firm - Column 2']))

        progress_bar.update(len(batch_data))

    # Close the cursor and connection
    cursor.close()
    conn.commit()
    conn.close()
    progress_bar.close()

# Call the function to fetch data in batches and process them
batch_size = 1000  # Adjust the batch size as per your requirement
fetch_data_in_batches(batch_size)
