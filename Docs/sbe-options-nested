import sqlite3
import xml.etree.ElementTree as ET
from multiprocessing import Pool, cpu_count
from tqdm import tqdm  # Import tqdm for the progress bar

# Function to determine Sub ID
def extract_sub_id(element):
    sub_id_element = element.find('.//fixml:Sub', ns)
    return sub_id_element.get('ID') if sub_id_element is not None else None

# Function to extract attribute values from the XML elements
def extract_attributes(element, attribute_names):
    return [element.get(attr) for attr in attribute_names]

# Function to process a batch of data and insert into the database
def process_batch(batch_data):
    conn = sqlite3.connect('trades.db')
    cursor = conn.cursor()

    # Insert the batch data into the SQLite table
    cursor.executemany('INSERT INTO trades VALUES (?, ?, ?, ?, ?, ?)', batch_data)
    conn.commit()

    cursor.close()
    conn.close()

# Function to fetch data in batches from the trades database
def fetch_data_in_batches(batch_size):
    # Connect to the SQLite database
    conn = sqlite3.connect('trades.db')
    cursor = conn.cursor()

    # Create the table for storing the trade data
    cursor.execute('''CREATE TABLE IF NOT EXISTS trades (
                        Quantity INTEGER,
                        Side TEXT,
                        Pty_ID TEXT,
                        Pty_R TEXT,
                        Sub_ID TEXT,
                        Rpt_ID TEXT
                     )''')

    # Define the namespace
    ns = {'fixml': 'http://www.fixprotocol.org/FIXML-4-4'}

    # Parse the XML data and extract the required attributes
    print("Parsing the XML data...")
    tree_trade = ET.parse('path_to_your_large_xml.xml')
    root_trade = tree_trade.getroot()

    batch_data = []  # List to store batch data
    total_rows = 0

    # Calculate the total number of rows for progress bar
    for trd_capt_rpt in root_trade.findall('.//fixml:TrdCaptRpt', ns):
        for rpt_side in trd_capt_rpt.findall('fixml:RptSide', ns):
            for _ in rpt_side.findall('fixml:Pty', ns):
                total_rows += 1

    progress_bar = tqdm(total=total_rows, desc="Inserting", unit="rows")

    # Loop through the TrdCaptRpt elements and extract the data
    print("Extracting data from the XML...")
    for trd_capt_rpt in root_trade.findall('.//fixml:TrdCaptRpt', ns):
        quantity, side, rpt_id = extract_attributes(trd_capt_rpt, ['LastQty', 'Side', 'RptID'])
        
        for rpt_side in trd_capt_rpt.findall('fixml:RptSide', ns):
            for pty in rpt_side.findall('fixml:Pty', ns):
                pty_id, pty_r, sub_id = extract_attributes(pty, ['ID', 'R']), extract_sub_id(pty)

                # Add the data to the batch_data list
                batch_data.append((quantity, side, pty_id, pty_r, sub_id, rpt_id))

                # Insert the batch data into the database when the batch size is reached
                if len(batch_data) >= batch_size:
                    with Pool(processes=cpu_count()) as pool:
                        pool.map(process_batch, [batch_data])

                    progress_bar.update(len(batch_data))
                    batch_data = []

    # Insert the remaining batch data into the database
    if batch_data:
        with Pool(processes=cpu_count()) as pool:
            pool.map(process_batch, [batch_data])

        progress_bar.update(len(batch_data))
        batch_data = []

    # Close the cursor and connection
    cursor.close()
    conn.close()

    progress_bar.close()
    print("Data has been successfully stored in the SQLite database.")

# Call the function to fetch data in batches and process them
batch_size = 1000  # Adjust the batch size as per your requirement
fetch_data_in_batches(batch_size)
