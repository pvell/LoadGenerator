import unittest
import time
import random
import configparser
import quickfix as fix
import json

# Global variables
sessions = {}

config = configparser.ConfigParser()
config.read("config.cfg")
log_file = config.get("DEFAULT", "FileLogPath")

def replace_placeholders(template, placeholders):
    for placeholder, value in placeholders.items():
        template = template.replace(placeholder, value)
    return template

def validate_execution_report(exec_report, order_qty, symbol, order_type, tif):
    # Validate OrderQty
    assert exec_report.getField(32) == str(order_qty), "Execution Report OrderQty mismatch"

    # Validate Symbol
    assert exec_report.getField(55) == symbol, "Execution Report Symbol mismatch"

    # Validate OrdType
    assert exec_report.getField(40) == order_type, "Execution Report OrdType mismatch"

    # Validate TimeInForce
    assert exec_report.getField(59) == tif, "Execution Report TimeInForce mismatch"

class MyApplication(fix.Application):
    def __init__(self):
        super().__init__()
        self.execution_report_received = False
        self.last_execution_report = None
        self.cl_ord_ids = []
        self.sessions = {}
        self.log_file = log_file

    def onCreate(self, sessionID):
        print("Session created -", sessionID.toString())
        self.sessions[sessionID] = fix.Session.lookupSession(sessionID)
        

    def onLogon(self, sessionID):
        print("Logon Successful -", sessionID.toString())

    def onLogout(self, sessionID):
        print("Logout initiated -", sessionID.toString())

    def toAdmin(self, message, sessionID):
        if message.getHeader().getField(fix.MsgType()).getString() == fix.MsgType_Logon:
            message.getHeader().setField(1408, "1.3")
            print("sent admin message", message.toString())

    def fromAdmin(self, message, sessionID):
        global sessions
        session_id = sessionID.toString()
        incoming_msg_seq_num = int(message.getHeader().getField(34))
        msg_type = message.getHeader().getField(35)
        print(message)

        if msg_type == 'A':  # Logon message
            if incoming_msg_seq_num == 1:
                print(f"Session established for {session_id}")
                sessions[session_id] = True
        elif msg_type == '5':  # Logout message
            print(f"Session disconnected for {session_id}")
            sessions[session_id] = False
        with open(self.log_file, 'a') as file:
            file.write(f"Received fromAdmin message:{message.toString()}\n")

    def toApp(self, message, sessionID):
        session_id = sessionID.toString()
        with open(self.log_file, "a") as file:
                file.write(f"Session: {session_id}\n")
                file.write(message.toString() + '\n')
        print("sent application message", message.toString())

    def fromApp(self, message, sessionID):
        msg_type = fix.MsgType()
        message.getHeader().getField(msg_type)

        if msg_type.getValue() == fix.MsgType_ExecutionReport:
            cl_ord_id = fix.ClOrdID()
            exec_report = fix.ExecutionReport()
            message.getField(cl_ord_id)
            message.getField(exec_report)
            self.last_execution_report = exec_report

            # Check if the execution report matches the conditions
            if exec_report.getField(39).getString() == '0' or exec_report.getField(39).getString() == '150':
                self.execution_report_received = True

    def generate_clordid(self):
        return str(random.randint(100000, 999999))

class TestFIXSession(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.app = MyApplication()
        cls.settings = fix.SessionSettings("config.cfg")
        cls.storeFactory = fix.FileStoreFactory(cls.settings)
        cls.logFactory = fix.ScreenLogFactory(cls.settings)
        cls.initiator = fix.SocketInitiator(cls.app, cls.storeFactory, cls.settings, cls.logFactory)
        cls.initiator.start()
        time.sleep(1)

    @classmethod
    def tearDownClass(cls):
        cls.initiator.stop()

    def send_new_order_single(self, order_type, time_in_force, order_qty, symbol, exec_inst=''):
        # Read the JSON file
        with open('instrument_definitions.json', 'r') as file:
            json_data = json.load(file)

        # Find the matching optionId based on the symbol
        option_id = None
        for item in json_data:
            if item['underlyingSymbolId'] == symbol:
                option_id = item['optionId']
                break

        if not option_id:
            raise ValueError(f"No optionId found for symbol: {symbol}")

        message = fix.Message()
        message.getHeader().setField(fix.BeginString(self.app.sessions[self.session_id].getBeginString().getString()))
        message.getHeader().setField(fix.MsgType(fix.MsgType_NewOrderSingle))
        message.getHeader().setField(fix.SenderCompID(self.app.sessions[self.session_id].getSenderCompID().getString()))
        message.getHeader().setField(fix.TargetCompID(self.app.sessions[self.session_id].getTargetCompID().getString()))
        message.getHeader().setField(fix.MsgSeqNum(self.app.get_outgoing_seq_num(self.session_id)))

        # Set other required fields for NewOrderSingle message
        message.setField(fix.Symbol(symbol))
        message.setField(fix.Side(fix.Side_BUY))
        message.setField(fix.OrderQty(order_qty))
        message.setField(fix.Price(8))
        message.setField(fix.OrdType(fix.OrdType_LIMIT))
        message.setField(fix.TimeInForce(time_in_force))
        message.setField(fix.ClOrdID(self.app.generate_clordid()))
        message.setField(fix.ExecInst(exec_inst))
        message.setField(1815, "6")
        message.setField(21035, option_id)
        message.setField(201, json_data[0]['putCall'])
        message.setField(202, str(json_data[0]['strikePrice']))
        message.setField(541, str(json_data[0]['expirationDate']))

        # Create the repeating group for PartyIDs
        party_group = fix.Group(fix.Tag_NoPartyIDs, fix.Tag_PartyID, [fix.Tag_PartyIDSource, fix.Tag_PartyRole])
        party_group.setField(fix.PartyID("QAX3"))
        party_group.setField(fix.PartyIDSource("D"))
        party_group.setField(fix.PartyRole("1"))
        message.addGroup(party_group)

        fix.Session.sendToTarget(message, self.session_id)

        time.sleep(2)

        self.assertTrue(self.app.execution_report_received)
        exec_report = self.app.last_execution_report
        cl_ord_id = exec_report.getField(11)

        validate_execution_report(exec_report, order_qty=order_qty, symbol=symbol, order_type=order_type, tif=time_in_force)
        self.assertEqual(exec_report.getField(39), '0')

    def send_cancel_replace_request(self, orig_cl_ord_id, new_price=None, new_order_qty=None):
        # Implementation for sending cancel/replace request
        pass

    def send_order_cancel_request(self, orig_cl_ord_id):
        # Implementation for sending order cancel request
        pass

    def test_market_order_with_day_tif(self):
        self.send_new_order_single(order_type='1', time_in_force='0', order_qty=100, symbol='AAPL')

    def test_market_order_with_ioc_tif(self):
        self.send_new_order_single(order_type='1', time_in_force='3', order_qty=100, symbol='AAPL')

    def test_limit_order_with_day_tif(self):
        self.send_new_order_single(order_type='2', time_in_force='0', order_qty=100, symbol='AAPL')

    def test_limit_order_with_ioc_tif(self):
        self.send_new_order_single(order_type='2', time_in_force='3', order_qty=100, symbol='AAPL')

    def test_limit_order_with_iso_modifier(self):
        self.send_new_order_single(order_type='2', time_in_force='0', order_qty=100, symbol='AAPL', exec_inst='F')

    def test_rejected_market_order_with_iso_modifier(self):
        self.send_new_order_single(order_type='1', time_in_force='0', order_qty=100, symbol='AAPL', exec_inst='F')
        self.assertEqual(self.app.last_execution_report.getField(39), '8')

    def test_cancel_replace_price(self):
        self.send_new_order_single(order_type='2', time_in_force='0', order_qty=100, symbol='AAPL')
        orig_cl_ord_id = self.app.last_execution_report.getField(11)
        self.send_cancel_replace_request(orig_cl_ord_id, new_price=150)

    def test_cancel_replace_qty(self):
        self.send_new_order_single(order_type='2', time_in_force='0', order_qty=100, symbol='AAPL')
        orig_cl_ord_id = self.app.last_execution_report.getField(11)
        self.send_cancel_replace_request(orig_cl_ord_id, new_order_qty=50)

    def tearDown(self):
        orig_cl_ord_id = self.app.last_execution_report.getField(11)
        self.send_order_cancel_request(orig_cl_ord_id)

if __name__ == '__main__':
    unittest.main()











class PartyIDSource:
    SIZE = 1

    def __init__(self, value):
        self.value = value

    def encode(self):
        return self.value.encode()

    def decode(self, buffer):
        self.value = buffer.decode()


class PartyRoleType:
    SIZE = 1

    def __init__(self, value):
        self.value = value

    def encode(self):
        return pack('>B', self.value)

    def decode(self, buffer):
        self.value = unpack_from('>B', buffer)[0]


class PartiesGroup:
    def __init__(self, party_ids=None):
        self.party_ids = party_ids or []
        print(party_ids)
     

    def encode(self):
        encoded_parties = b''.join(b''.join(party_obj.encode() for party_obj in nested_list) for nested_list in self.party_ids)
        return encoded_parties

    def decode(self, buffer):
        num_party_ids = len(buffer) // (PartyID.SIZE + PartyIDSource.SIZE + PartyRoleType.SIZE)
        self.party_ids = []
        for i in range(num_party_ids):
            offset = i * (PartyID.SIZE + PartyIDSource.SIZE + PartyRoleType.SIZE)
            party_id = PartyID('')
            party_id.decode(buffer[offset:offset + PartyID.SIZE])
            offset += PartyID.SIZE
            party_id_source = PartyIDSource('')
            party_id_source.decode(buffer[offset:offset + PartyIDSource.SIZE])
            offset += PartyIDSource.SIZE
            party_role = PartyRoleType('')
            party_role.decode(buffer[offset:offset + PartyRoleType.SIZE])
            self.party_ids.append([party_id, party_id_source, party_role])





class RepeatingGroupDimensions:
    SIZE = 2

    def __init__(self, block_length,num_groups):
        self.block_length = block_length
        self.num_groups = num_groups

    def encode(self):
        return pack('>BB', self.block_length, self.num_groups)

    def decode(self, buffer):
        self.block_length,self.num_groups = unpack_from('>BB', buffer)



def encode(self):
    encoded_parties = b''.join(party_obj.encode() for nested_list in self.party_ids for party_obj in nested_list)
    return encoded_parties


Traceback (most recent call last):
  File "sbe_load_generator.py", line 470, in <module>
    main()
  File "sbe_load_generator.py", line 451, in main
    message = generate_message(message_type,session_name)
  File "sbe_load_generator.py", line 191, in generate_message
    encoded_message = new_order_single.encode()
  File "/home/pvellanki/myworkspace/SBE/sbe_encoder_decoder.py", line 534, in encode
    encoded_RepeatingGroupDimensions +
TypeError: can't concat builtin_function_or_method to bytes
[pvellanki@sys0412 SBE]$ 




Side	Sub ID	RptID	Ultimate Clearing Firm	Entering Clearing Frim/Contra Clearing Firm	Executing Broker/Position Account Type	Quantity
1	C	240150750		255		141
2	C	240150750		226		141
1	C	240150751		255		76
2	C	240150751		273		76


pvellanki@pvellanki-MacBookPro orsa % python3 gen_orsa_report_v3.py 
Traceback (most recent call last):
  File "/Users/pvellanki/orsa/gen_orsa_report_v3.py", line 131, in <module>
    df.to_excel(writer, sheet_name=f'Chunk_{chunk_num}', index=False)
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_base.py", line 1322, in __exit__
    self.close()
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_base.py", line 1326, in close
    self._save()
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_openpyxl.py", line 109, in _save
    self.book.save(self._handles.handle)
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/openpyxl/workbook/workbook.py", line 386, in save
    save_workbook(self, filename)
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/openpyxl/writer/excel.py", line 294, in save_workbook
    writer.save()
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/openpyxl/writer/excel.py", line 275, in save
    self.write_data()
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/openpyxl/writer/excel.py", line 89, in write_data
    archive.writestr(ARC_WORKBOOK, writer.write())
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/openpyxl/workbook/_writer.py", line 150, in write
    self.write_views()
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/openpyxl/workbook/_writer.py", line 137, in write_views
    active = get_active_sheet(self.wb)
  File "/Users/pvellanki/Library/Python/3.9/lib/python/site-packages/openpyxl/workbook/_writer.py", line 35, in get_active_sheet
    raise IndexError("At least one sheet must be visible")
IndexError: At least one sheet must be visible



class PartiesGroup:
    # ... other methods ...

    def encode(self):
        encoded_parties = b''
        for party in self.party_ids:
            for party_obj in party:
                encoded_parties += party_obj.encode()
        return encoded_parties



pvellanki@qa2434:~/Final/SBE$ pwd
/home/pvellanki/Final/SBE


import xml.etree.ElementTree as ET
import os

# Function to split the XML tree into chunks
def split_xml_tree(xml_tree, chunk_size):
    root = xml_tree.getroot()
    total_elements = len(root)
    chunk_elements = total_elements // chunk_size
    remaining_elements = total_elements % chunk_size

    chunks = []
    start = 0
    for i in range(chunk_size):
        chunk_length = chunk_elements + (1 if i < remaining_elements else 0)
        chunk = ET.ElementTree(ET.Element('Root'))
        chunk.getroot().extend(root[start:start + chunk_length])
        chunks.append(chunk)
        start += chunk_length

    return chunks

# Read the original XML data from trade.xml and parse it into an ElementTree (replace 'trade.xml' with the actual file path)
tree_trade = ET.parse('trade_bak.xml')

# Split the XML tree into 4 chunks
chunked_trees = split_xml_tree(tree_trade, chunk_size=4)

# Create a directory to store the split XML files
output_directory = 'split_xml'
if not os.path.exists(output_directory):
    os.makedirs(output_directory)

# Save each chunk as a separate XML file
for i, chunk_tree in enumerate(chunked_trees):
    output_file_name = os.path.join(output_directory, f'part_{i+1}.xml')
    chunk_tree.write(output_file_name)

print("XML file has been split into 4 parts.")


Parsing the XML data...
Filtering the DataFrame...
Traceback (most recent call last):
  File "/Users/pvellanki/orsa/gen_orsa_report_v5.py", line 152, in <module>
    filtered_trade_data_frame = trade_data_frame[
NameError: name 'trade_data_frame' is not defined



import xml.etree.ElementTree as ET

# Function to split a list into approximately equal parts
def split_list(lst, num_parts):
    avg = len(lst) / float(num_parts)
    out = []
    last = 0.0

    while last < len(lst):
        out.append(lst[int(last):int(last + avg)])
        last += avg

    return out

def main():
    # Load and parse the original XML file
    tree = ET.parse("original.xml")
    root = tree.getroot()
    trd_capt_rpt_elements = root.findall(".//{http://www.fixprotocol.org/FIXML-4-4}TrdCaptRpt")

    # Split the TrdCaptRpt elements into 4 parts
    num_parts = 4
    trd_capt_rpt_parts = split_list(trd_capt_rpt_elements, num_parts)

    # Create and write each part into separate XML files
    for i, part in enumerate(trd_capt_rpt_parts):
        part_root = ET.Element("FIXML", attrib=root.attrib, nsmap=root.nsmap)
        batch_element = ET.SubElement(part_root, "Batch")
        batch_element.extend(part)

        part_tree = ET.ElementTree(part_root)
        part_tree.write(f"batch_{i + 1}.xml", xml_declaration=True, encoding="utf-8")

if __name__ == "__main__":
    main()










Summary
The current implementation of wheelhouse pushes most of the computation onto the client-side javascript. To do this, it pushes all status updates to all connected clients. Status updates are fairly large JSON blocks, and this creates a lot of I/O for the server and client to process. Reduce the amount of IO between the server and client by moving the processing of the table contents to the server side, along with probe and condition checking.

Client-Server Connection
The current connection is a websocket connection. This was chosen as opposed to a RESTful API with a polling client to allow the client to be able to display whether it had connectivity to the server and not have to re-pull the entire status list on each poll. The new mechanism should continue to use a websocket for client communication.

The backend server should only emit enough information to allow the client to render the table it wants to display (currently, there is only one table, but we will update that in a future enhancement). The server should allow the client to subscribe/unsubscribe for table data, and for individual status updates for specific keys. The client should only subscribe for what it needs to display. 

Modify the protocol between the clients and the backend to be the equivalent of the following operations (NOTE: the message IDs shown below do not have to be sent over the wire as full strings - they are simply defined that way below to identify the operation in this document. These can be sent over the wire, for example, as a single char, a binary value, etc - once that decision is made, we can update this page).

Operations from client-to-server:
“list_tables” - requests a list of all available status tables from the server. The existing table in the current Wheelhouse implementation will be maintained on the backend as table name “default”. Another ticket will be for the ability to create other tables on the backend via config that have custom column sets and filters, but this is out of scope for the initial performance-related change. 

“subscribe_table <table_name>” - subscribes to a table being maintained on the backend. Subscription to a table only sends enough information to the client to allow the client to render the table properly and perform the regex filtering on the table contents. DO NOT do regex filtering of the table server-side.

“unsubscribe_table <table_name>” - informs the server to stop sending updates on the table name provided. 

“unsubscribe_all_tables” - informs the server to stop sending updates for all currently subscribed tables

“subscribe_status <status_key>” - subscribes to the full status updates for a particular key. Will initially reply with an error (if unknown key, etc) or an accept containing a snapshot of the current status json for the key. Future updates received on that key will be sent as updates. 

“unsubscribe_status <status_key>” - unsubscribes from updates on a particular key. 

“unsubscribe_all_status” - unsubscribes from all existing status subscriptions

Operations from server-to-client:
“table_subscription_rejected <table_name> <reason>” 

“table_subscription_accepted <table_name> <table_meta_data> <table_data_snapshot>” 

“table_subscription_update <table_name> <table_data_delta>”

“table_subscription_closed <table_name>”

“status_subscription_rejected <key> <reason>”

“status_subscription_accepted <key> <current_status>”

“status_subscription_update <key> <new_status>”

“status_subscription_closed <key> <reason>” - reason can be “requested” or “key deleted”, etc.

 

Message Payloads
table_subscription_accepted
This message is sent when a client is successfully subscribed to a table and has the following format:


{
  "name" : "table_name",
  "meta_data" : { 
    //table meta data - see below
  },
  "rows" : [
    //0 or more table row information objects - see below
  ]
}
table_meta_data
This is used to provide the client with information on how to render the table data for this table. For now this only contains the column headers. Later, other rendering instructions may be included.


{
  "columns" : [
    { "name" : "Foo" },
    { "name" : "Bar" },
    { "name" : "Status" }    
  ]
}
table_data_snapshot
This contains all of the row information for the table. It should be considered a list of “row add” commands. “Key” is never a rendered column, but rather a unique key identifying a row in the table, and also the key used to subscribe for individual status if needed (when clicking “view status”, etc). All Records in this array are in the “row” object format: 


{
  "key" : "gc1234.app_name_1.ACTOR_NAME_1",
  "bgcolor" : "green", 
  "fgcolor" : null,   //null or not present means "default fgcolor, otherwise an html color code
  "tooltip" : null,
  "data" : {
      "Foo" : "foo_value",
      "Bar" : 1234,
      "Status" : "OK"
  },
  "badges" : {
      "Status" : [ "Connected", "Active" ]
  }
}
Fields:

”key” - Each row entry in the table has a unique “key”. This key identifies the row in all subsequent updates, and may be used by the client in “subscribe_status”, and “unsubscribe_status” requests to get the full detailed status json for that item. 

“bgcolor” - informs the client what color to make the background for this row. If this value is null or not present, it means use the default background color of the page itself.

“fgcolor” - informs the client what color to make the foreground for this row. If this value is null or not present, it means use the default foreground color of the page itself

“tooltip” - informs the client what to set the text of the tooltip to on the row. If null or not present, it means no tooltip.

“data” - this object’s fields will be named the same as the Columns defined in the Table Meta Data for this table, and their values will be the value to display for that column on the page.

“badges” - this objects fields will be named the same as the Columns defined in the Table Meta Data for the table, and the values will be arrays of “Badge” strings to show in that column after the standard data text.

 

table_subscription_update
This is sent when the backend has calculated an update to a particular table’s contents and needs to inform any subscribed client(s).

The structure of this update will be as follows:


{
  "delete" : [
    //list containing keys of all rows to be deleted
  ],
  "add" : [
    //list of new row objects as provided in the initial snapshot to add to the table
  ]
  "updates" : [
    //list of updated row objects containing only the deltas to the last published values - see below  
  ]
}
The Delete list above is simply a json array of keys. The keys are the keys of the rows that are to be deleted from the table.

The Add list above is a list of row objects, exactly as provided in the original snapshot, of rows to add to the table.

The update list above contains a list of DELTA row objects. The structure of these objects is the same as in the snapshot and add, but the behavior of these objects is slightly different as follows:

All update objects MUST contain a “key” field.

All update objects SHOULD omit any field that has not changed since the last update

All update objects that wish to “clear” a field should explicitly set it to null (for example, clearing a tooltip on a row that was in error but is now no longer in error).

If a column has not changed, it does not need to be provided in the “data” object - only columns that have changed since the last update need to be provided in the “data” object










Development
User Stories
As a developer assisting operations, I need to be able to view the complete status for a specific actor, so that I can determine the state of an actor.

As a developer assisting operations, I need to be able to view the list of actors, so that I can understand what was deployed.

Techops
User Stories
As an operator, I need to be able to see which actors are in ERROR or WARNING state, so that I can detect and triage system issues.

As an operator, I need to be able to see what system conditions have been triggered, so that I can detect and triage system issues.

As an operator, I need to be able to modify and update probes in an ongoing basis, so that I can reduce  spurious warnings and detect newly discovered issues.

As an operator, I need to be able to filter for actors based on host, so that I can determine impact of issues affecting certain environments or hosts.

As an operator, I need to be able to filter for actors based on their type (e.g: MEMO port, FIX port, republisher, etc.), so that I can locate specific instances whose identifiers might be unknown.

As an operator, I need to know the total number of actors that are successfully returning status to Wheelhouse. This will give me a view if we are missing a subset of actors and/or if a status_collector in a datacenter is having issue connecting. 

Detail
Adding the most and least used features of wheelhouse from TechOps perspective.

Least used - Explore 

Most used - 

Page displaying actors in a state of ERRORs and/or WARNINGs 


2. Page displaying all probes and conditions triggered -


3. Ability to add probes and conditions in WH.

4. Filter feature based on the following criteria -

Host (gc0408.ny4.memx.local)

Class of actors

memo, fix, sbe

republisher, serializer

 



